{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Description \u00b6 This webpage contains the documentation necessary to run multiple pipelines developped for the NCI-RBL.","title":"Intro"},{"location":"#description","text":"This webpage contains the documentation necessary to run multiple pipelines developped for the NCI-RBL.","title":"Description"},{"location":"DTEG/descript/","text":"DTEG \u00b6 This pipeline integrates RNA-seq and Ribo-seq to calculate translation efficiency as the number of ribosomes per gene, normalized to transcript abundance. Genes with significant changes in translation efficiency between conditions are considered to undergo translational regulation and are labelled Differential Translation Efficiency Genes (DTEGs). See the original publication and GitHub repository for more information. To run this pipeline, you will need to have NextFlow and Singularity installed (already available on Biowulf and FRCE). For more questions about this wrapper of the DTEG pipeline, you may contact Colin Wu or Wilfried Guiblet .","title":"Description"},{"location":"DTEG/descript/#dteg","text":"This pipeline integrates RNA-seq and Ribo-seq to calculate translation efficiency as the number of ribosomes per gene, normalized to transcript abundance. Genes with significant changes in translation efficiency between conditions are considered to undergo translational regulation and are labelled Differential Translation Efficiency Genes (DTEGs). See the original publication and GitHub repository for more information. To run this pipeline, you will need to have NextFlow and Singularity installed (already available on Biowulf and FRCE). For more questions about this wrapper of the DTEG pipeline, you may contact Colin Wu or Wilfried Guiblet .","title":"DTEG"},{"location":"DTEG/init/","text":"Prepare directories \u00b6 Create a working directory and subdirectories: mkdir DTEG cd DTEG mkdir FASTQ mkdir HTSeq Create a subdirectory for each sample. Example: mkdir FASTQ/HEK_WT1 mkdir FASTQ/HEK_WT2 mkdir FASTQ/HEK_ThumD_KO1 mkdir FASTQ/HEK_ThumD_KO2 mkdir FASTQ/HEK293ThumpD1KO1RNA mkdir FASTQ/HEK293ThumpD1KO2RNA mkdir FASTQ/HEK293WT1RNA mkdir FASTQ/HEK293WT2RNA Index \u00b6 You can link the pre-made index folder for hg19 in the working directory: ln -s /mnt/rnabl-work/Guiblet/CCBRRBL8/NextFlow ./ You can create a new index as follow: TBA Scripts \u00b6 Download the following files in the working directory: DTEG_RBL.nf DTEG_RBL.sh nextflow.config Open the nextflow.config file and change /mnt/rnabl-work/Guiblet/CCBRRBL8/NextFlow/ to your working directory (full path) Sample Info \u00b6 Create the sample_info.txt as the following example: SampleID Condition SeqType Batch HEK_WT1 1 RIBO 1 HEK_WT2 1 RIBO 2 HEK_ThumD_KO1 2 RIBO 1 HEK_ThumD_KO2 2 RIBO 2 HEK293WT1RNA 1 RNA 1 HEK293WT2RNA 1 RNA 2 HEK293ThumpD1KO1RNA 2 RNA 1 HEK293ThumpD1KO2RNA 2 RNA 2","title":"Initialize"},{"location":"DTEG/init/#prepare-directories","text":"Create a working directory and subdirectories: mkdir DTEG cd DTEG mkdir FASTQ mkdir HTSeq Create a subdirectory for each sample. Example: mkdir FASTQ/HEK_WT1 mkdir FASTQ/HEK_WT2 mkdir FASTQ/HEK_ThumD_KO1 mkdir FASTQ/HEK_ThumD_KO2 mkdir FASTQ/HEK293ThumpD1KO1RNA mkdir FASTQ/HEK293ThumpD1KO2RNA mkdir FASTQ/HEK293WT1RNA mkdir FASTQ/HEK293WT2RNA","title":"Prepare directories"},{"location":"DTEG/init/#index","text":"You can link the pre-made index folder for hg19 in the working directory: ln -s /mnt/rnabl-work/Guiblet/CCBRRBL8/NextFlow ./ You can create a new index as follow: TBA","title":"Index"},{"location":"DTEG/init/#scripts","text":"Download the following files in the working directory: DTEG_RBL.nf DTEG_RBL.sh nextflow.config Open the nextflow.config file and change /mnt/rnabl-work/Guiblet/CCBRRBL8/NextFlow/ to your working directory (full path)","title":"Scripts"},{"location":"DTEG/init/#sample-info","text":"Create the sample_info.txt as the following example: SampleID Condition SeqType Batch HEK_WT1 1 RIBO 1 HEK_WT2 1 RIBO 2 HEK_ThumD_KO1 2 RIBO 1 HEK_ThumD_KO2 2 RIBO 2 HEK293WT1RNA 1 RNA 1 HEK293WT2RNA 1 RNA 2 HEK293ThumpD1KO1RNA 2 RNA 1 HEK293ThumpD1KO2RNA 2 RNA 2","title":"Sample Info"},{"location":"DTEG/run/","text":"Run pipeline: \u00b6 Simply submit the pipeline as a slurm job: sbatch DTEG_RBL.sh If you do not have slurm and/or are running the pipeline on a local computer (not recommended), you may run the different parts of the workflow as follow: nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process AlignRNA nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process RunRiboSeq nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process RunHTseq nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process MergeCounts nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process DTEG Running this pipeline will download the Docker container in a folder named \"work\". To remove the container, delete the folder. rm -r work","title":"Run"},{"location":"DTEG/run/#run-pipeline","text":"Simply submit the pipeline as a slurm job: sbatch DTEG_RBL.sh If you do not have slurm and/or are running the pipeline on a local computer (not recommended), you may run the different parts of the workflow as follow: nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process AlignRNA nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process RunRiboSeq nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process RunHTseq nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process MergeCounts nextflow run -c nextflow.config test.nf --SampleInfo sample_info.txt --Process DTEG Running this pipeline will download the Docker container in a folder named \"work\". To remove the container, delete the folder. rm -r work","title":"Run pipeline:"},{"location":"ISOMIR/descript/","text":"ISOMIR \u00b6 Steps of the pipeline: Adapter removal Small RNA profiling QuagmiR analysis For more questions about this wrapper of the ISOMIR pipeline, you may contact Shuo Guo or Wilfried Guiblet .","title":"Description"},{"location":"ISOMIR/descript/#isomir","text":"Steps of the pipeline: Adapter removal Small RNA profiling QuagmiR analysis For more questions about this wrapper of the ISOMIR pipeline, you may contact Shuo Guo or Wilfried Guiblet .","title":"ISOMIR"},{"location":"ISOMIR/init/","text":"Prepare directories \u00b6 Create a working directory and subdirectories: mkdir ISOMIR cd ISOMIR mkdir fastq_files Move all your experimental FastQ files in the fastq_files directory. Index \u00b6 You will need an Index directory for mapping to rRNA, tRNA, snoRNA, miRNA, transcriptome, and mycoplasma. You can download this Index for hg38. Extract the downloaded directory : tar zxvf index.tar.gz Scripts \u00b6 Download the following files in the working directory: isomiR.nf isomiR.sh nextflow.config motif-consensus.fa","title":"Initialize"},{"location":"ISOMIR/init/#prepare-directories","text":"Create a working directory and subdirectories: mkdir ISOMIR cd ISOMIR mkdir fastq_files Move all your experimental FastQ files in the fastq_files directory.","title":"Prepare directories"},{"location":"ISOMIR/init/#index","text":"You will need an Index directory for mapping to rRNA, tRNA, snoRNA, miRNA, transcriptome, and mycoplasma. You can download this Index for hg38. Extract the downloaded directory : tar zxvf index.tar.gz","title":"Index"},{"location":"ISOMIR/init/#scripts","text":"Download the following files in the working directory: isomiR.nf isomiR.sh nextflow.config motif-consensus.fa","title":"Scripts"},{"location":"ISOMIR/run/","text":"Run pipeline: \u00b6 Simply submit the pipeline as a slurm job: sbatch isomiR.sh <ExperimentName> \"<Options>\" Example run: sbatch isomiR.sh Name \"--trimmer Qiagen --MinLen 18 --consensus motif-consensus.fa --index hg38\" List of options. --trimmer : Trimmer used. Default is Qiagen. Qiagen Illumina NEB GuLab None --MinLen: Minimum length for trimming. Requires an integer. Default is 18. --consensus : alternative consensus file. --index : Index for profiling. Currently available: hg38 mm39 --sRNAprofiling : Yes or No. No skips this step. --QuagmiR : Yes or No. No skips this step.","title":"Run"},{"location":"ISOMIR/run/#run-pipeline","text":"Simply submit the pipeline as a slurm job: sbatch isomiR.sh <ExperimentName> \"<Options>\" Example run: sbatch isomiR.sh Name \"--trimmer Qiagen --MinLen 18 --consensus motif-consensus.fa --index hg38\" List of options. --trimmer : Trimmer used. Default is Qiagen. Qiagen Illumina NEB GuLab None --MinLen: Minimum length for trimming. Requires an integer. Default is 18. --consensus : alternative consensus file. --index : Index for profiling. Currently available: hg38 mm39 --sRNAprofiling : Yes or No. No skips this step. --QuagmiR : Yes or No. No skips this step.","title":"Run pipeline:"},{"location":"LEAFCUTTER/descript/","text":"LEAFCUTTER \u00b6 Tutorial to run leafcutter. https://github.com/davidaknowles/leafcutter https://davidaknowles.github.io/leafcutter/ Comes with a docker container for leafcutter itself. For more questions about this tutorial, you may contact Wilfried Guiblet .","title":"Description"},{"location":"LEAFCUTTER/descript/#leafcutter","text":"Tutorial to run leafcutter. https://github.com/davidaknowles/leafcutter https://davidaknowles.github.io/leafcutter/ Comes with a docker container for leafcutter itself. For more questions about this tutorial, you may contact Wilfried Guiblet .","title":"LEAFCUTTER"},{"location":"LEAFCUTTER/init/","text":"Input files \u00b6 You will need: A STAR index directory (example: STAR_hg38) A genome annotation (example: gencode.v44.annotation.gtf) FASTQ files (we will assume paired files for this tutorial) Index \u00b6 If you do not have a STAR index you can create one: STAR \\ --runThreadN 64 \\ --runMode genomeGenerate \\ --genomeDir STAR_hg38/ \\ --genomeFastaFiles references/hg38.fa \\ --sjdbGTFfile references/gencode.v44.annotation.gtf","title":"Initialize"},{"location":"LEAFCUTTER/init/#input-files","text":"You will need: A STAR index directory (example: STAR_hg38) A genome annotation (example: gencode.v44.annotation.gtf) FASTQ files (we will assume paired files for this tutorial)","title":"Input files"},{"location":"LEAFCUTTER/init/#index","text":"If you do not have a STAR index you can create one: STAR \\ --runThreadN 64 \\ --runMode genomeGenerate \\ --genomeDir STAR_hg38/ \\ --genomeFastaFiles references/hg38.fa \\ --sjdbGTFfile references/gencode.v44.annotation.gtf","title":"Index"},{"location":"LEAFCUTTER/run/","text":"Run STAR: \u00b6 Align your samples in twopassMode for more accurate junction calling: STAR \\ --runThreadN 64 \\ --runMode alignReads \\ --genomeDir 'STAR_hg38' \\ --sjdbGTFfile 'gencode.v44.annotation.gtf' \\ --outFileNamePrefix ${ sample } _ \\ --outSAMunmapped Within \\ --outFilterType BySJout \\ --outFilterMultimapNmax 20 \\ --outFilterMismatchNmax 999 \\ --outFilterMismatchNoverLmax 0 .04 \\ --alignIntronMin 20 \\ --alignIntronMax 50000 \\ --alignMatesGapMax 1000000 \\ --alignSJoverhangMin 5 \\ --alignSJDBoverhangMin 3 \\ --sjdbScore 1 \\ --readFilesIn ${ sample } _R1.fq \\ ${ sample } _R2.fq \\ --outFilterMatchNminOverLread 0 .66 \\ --outSAMtype BAM Unsorted \\ --quantMode TranscriptomeSAM \\ --peOverlapNbasesMin 10 \\ --alignEndsProtrude 10 ConcordantPair \\ --twopassMode Basic \\ --outSAMstrandField intronMotif samtools sort -@ 64 -o ${ sample } _Aligned.sorted.bam ${ sample } _Aligned.out.bam samtools index -@ 64 -M ${ sample } _Aligned.sorted.bam Run regtools \u00b6 for bamfile in ` ls *_Aligned.sorted.bam ` ; do echo Converting $bamfile to $bamfile .junc regtools junctions extract -s XS -a 8 -m 50 -M 500000 $bamfile -o $bamfile .junc done Repeat alignment for all samples. Leafcutter documentation recommends 4 samples per group mimimum. Download and Run leafcutter container \u00b6 export SINGULARITY_CACHEDIR = $PWD /.singularity singularity pull --dir ./ leafcutter.sif docker://wilfriedguiblet/leafcutter:v0.1 Create a juncfiles.txt. Example: Sample1_Aligned.sorted.bam.junc Sample2_Aligned.sorted.bam.junc Sample3_Aligned.sorted.bam.junc Sample4_Aligned.sorted.bam.junc Sample5_Aligned.sorted.bam.junc Sample6_Aligned.sorted.bam.junc Sample7_Aligned.sorted.bam.junc Sample8_Aligned.sorted.bam.junc Download custom leafcutter: git clone https://github.com/wilfriedguiblet/leafcutter.git Enter the container singularity shell -B $( pwd ) /:/data2/,/data/RBL_NCI/:/data/RBL_NCI/ leafcutter.sif Cluster juncfiles python leafcutter/clustering/leafcutter_cluster_regtools.py -j juncfiles.txt -m 50 -o Experiment -l 500000 Prep annotation for leafcutter leafcutter/leafviz/gtf2leafcutter.pl -o gencode.v44 gencode.v44.annotation.gtf Create groups_file.txt. Example: Sample1_Aligned.sorted.bam WT Sample2_Aligned.sorted.bam WT Sample3_Aligned.sorted.bam WT Sample4_Aligned.sorted.bam WT Sample5_Aligned.sorted.bam KO Sample6_Aligned.sorted.bam KO Sample7_Aligned.sorted.bam KO Sample8_Aligned.sorted.bam KO Run leafcutter: Rscript leafcutter/scripts/leafcutter_ds.R --num_threads 64 Experiment_perind_numers.counts.gz groups_file.txt --min_samples_per_intron 4 --min_samples_per_group 4 -o Experiment Create RData file for leafviz: Rscript ../../leafcutter/leafviz/prepare_results.R --meta_data_file groups_file.txt \\ --code leafcutter Experiment_perind_numers.counts.gz \\ Experiment_cluster_significance.txt \\ Experiment_effect_sizes.txt \\ annotation_codes/gencode_v44/gencode.v44 \\ -o Experiment.RData Exit container. Run leafviz locally (for instance with RStudio) options ( shiny.host = \"0.0.0.0\" ) options ( shiny.port = 3838 ) library ( leafviz ) leafviz ( \"~/Lab_Work/CCRRBL-5/leafviz/Experiment.RData\" )","title":"Run"},{"location":"LEAFCUTTER/run/#run-star","text":"Align your samples in twopassMode for more accurate junction calling: STAR \\ --runThreadN 64 \\ --runMode alignReads \\ --genomeDir 'STAR_hg38' \\ --sjdbGTFfile 'gencode.v44.annotation.gtf' \\ --outFileNamePrefix ${ sample } _ \\ --outSAMunmapped Within \\ --outFilterType BySJout \\ --outFilterMultimapNmax 20 \\ --outFilterMismatchNmax 999 \\ --outFilterMismatchNoverLmax 0 .04 \\ --alignIntronMin 20 \\ --alignIntronMax 50000 \\ --alignMatesGapMax 1000000 \\ --alignSJoverhangMin 5 \\ --alignSJDBoverhangMin 3 \\ --sjdbScore 1 \\ --readFilesIn ${ sample } _R1.fq \\ ${ sample } _R2.fq \\ --outFilterMatchNminOverLread 0 .66 \\ --outSAMtype BAM Unsorted \\ --quantMode TranscriptomeSAM \\ --peOverlapNbasesMin 10 \\ --alignEndsProtrude 10 ConcordantPair \\ --twopassMode Basic \\ --outSAMstrandField intronMotif samtools sort -@ 64 -o ${ sample } _Aligned.sorted.bam ${ sample } _Aligned.out.bam samtools index -@ 64 -M ${ sample } _Aligned.sorted.bam","title":"Run STAR:"},{"location":"LEAFCUTTER/run/#run-regtools","text":"for bamfile in ` ls *_Aligned.sorted.bam ` ; do echo Converting $bamfile to $bamfile .junc regtools junctions extract -s XS -a 8 -m 50 -M 500000 $bamfile -o $bamfile .junc done Repeat alignment for all samples. Leafcutter documentation recommends 4 samples per group mimimum.","title":"Run regtools"},{"location":"LEAFCUTTER/run/#download-and-run-leafcutter-container","text":"export SINGULARITY_CACHEDIR = $PWD /.singularity singularity pull --dir ./ leafcutter.sif docker://wilfriedguiblet/leafcutter:v0.1 Create a juncfiles.txt. Example: Sample1_Aligned.sorted.bam.junc Sample2_Aligned.sorted.bam.junc Sample3_Aligned.sorted.bam.junc Sample4_Aligned.sorted.bam.junc Sample5_Aligned.sorted.bam.junc Sample6_Aligned.sorted.bam.junc Sample7_Aligned.sorted.bam.junc Sample8_Aligned.sorted.bam.junc Download custom leafcutter: git clone https://github.com/wilfriedguiblet/leafcutter.git Enter the container singularity shell -B $( pwd ) /:/data2/,/data/RBL_NCI/:/data/RBL_NCI/ leafcutter.sif Cluster juncfiles python leafcutter/clustering/leafcutter_cluster_regtools.py -j juncfiles.txt -m 50 -o Experiment -l 500000 Prep annotation for leafcutter leafcutter/leafviz/gtf2leafcutter.pl -o gencode.v44 gencode.v44.annotation.gtf Create groups_file.txt. Example: Sample1_Aligned.sorted.bam WT Sample2_Aligned.sorted.bam WT Sample3_Aligned.sorted.bam WT Sample4_Aligned.sorted.bam WT Sample5_Aligned.sorted.bam KO Sample6_Aligned.sorted.bam KO Sample7_Aligned.sorted.bam KO Sample8_Aligned.sorted.bam KO Run leafcutter: Rscript leafcutter/scripts/leafcutter_ds.R --num_threads 64 Experiment_perind_numers.counts.gz groups_file.txt --min_samples_per_intron 4 --min_samples_per_group 4 -o Experiment Create RData file for leafviz: Rscript ../../leafcutter/leafviz/prepare_results.R --meta_data_file groups_file.txt \\ --code leafcutter Experiment_perind_numers.counts.gz \\ Experiment_cluster_significance.txt \\ Experiment_effect_sizes.txt \\ annotation_codes/gencode_v44/gencode.v44 \\ -o Experiment.RData Exit container. Run leafviz locally (for instance with RStudio) options ( shiny.host = \"0.0.0.0\" ) options ( shiny.port = 3838 ) library ( leafviz ) leafviz ( \"~/Lab_Work/CCRRBL-5/leafviz/Experiment.RData\" )","title":"Download and Run leafcutter container"},{"location":"MOP2/descript/","text":"MOP2 \u00b6 This document describes how to use the Master Of Pores V2 (MOP2) pipeline on the FRCE server. MOP2 estimates the length of mRNA poly-A tail from Nanopore reads using Tailfindr and Nanopolish. See the original publication and GitHub repository for more information.","title":"Description"},{"location":"MOP2/descript/#mop2","text":"This document describes how to use the Master Of Pores V2 (MOP2) pipeline on the FRCE server. MOP2 estimates the length of mRNA poly-A tail from Nanopore reads using Tailfindr and Nanopolish. See the original publication and GitHub repository for more information.","title":"MOP2"},{"location":"MOP2/init/","text":"Prepare directories \u00b6 Create a working directory and subdirectories: mkdir MOP2 cd MOP2 mkdir MOP2_work mkdir MOP2_output Scripts and dependencies \u00b6 Copy the following files and folders in your working directory: cp /scratch/cluster_scratch/guibletwm/MOP2_repo/ ./ Download the following files in the working directory: PolyATail.sh Paths \u00b6 Modify the following paths in PolyATail.sh: basedir nextflow procdir","title":"Initialize"},{"location":"MOP2/init/#prepare-directories","text":"Create a working directory and subdirectories: mkdir MOP2 cd MOP2 mkdir MOP2_work mkdir MOP2_output","title":"Prepare directories"},{"location":"MOP2/init/#scripts-and-dependencies","text":"Copy the following files and folders in your working directory: cp /scratch/cluster_scratch/guibletwm/MOP2_repo/ ./ Download the following files in the working directory: PolyATail.sh","title":"Scripts and dependencies"},{"location":"MOP2/init/#paths","text":"Modify the following paths in PolyATail.sh: basedir nextflow procdir","title":"Paths"},{"location":"MOP2/run/","text":"Run pipeine: \u00b6 Use the command: bash PolyATail.sh /path_to_fast5_files/ The script will ask for a short project name. Intermediary files will be stored in the MOP2_work folder. PolyA-tail lengths will be copied in the MOP2_output. ~","title":"Run"},{"location":"MOP2/run/#run-pipeine","text":"Use the command: bash PolyATail.sh /path_to_fast5_files/ The script will ask for a short project name. Intermediary files will be stored in the MOP2_work folder. PolyA-tail lengths will be copied in the MOP2_output. ~","title":"Run pipeine:"},{"location":"RiboFootPrint/descript/","text":"RiboFootPrint \u00b6 Steps of the pipeline: Trimm Ribo-seq reads. Filter out reads mapping to non-coding RNA and remap (STAR) to transcriptome. Aggregates relative postions (range: 0-1) of read starts, split in 5'UTR, CDS, and 3'UTR. For more questions about this pipeline, you may contact Colin Wu or Wilfried Guiblet .","title":"Description"},{"location":"RiboFootPrint/descript/#ribofootprint","text":"Steps of the pipeline: Trimm Ribo-seq reads. Filter out reads mapping to non-coding RNA and remap (STAR) to transcriptome. Aggregates relative postions (range: 0-1) of read starts, split in 5'UTR, CDS, and 3'UTR. For more questions about this pipeline, you may contact Colin Wu or Wilfried Guiblet .","title":"RiboFootPrint"},{"location":"RiboFootPrint/init/","text":"Scripts \u00b6 Download all scripts from RiboFootPrint GitHub repository Index \u00b6 You can link the pre-made index folder for hg19 in the working directory: ln -s /mnt/rnabl-work/Guiblet/CCRRBL12/index ./ Set Parameters \u00b6 Update the file: RiboFootPrint.parameters.yaml","title":"Initialize"},{"location":"RiboFootPrint/init/#scripts","text":"Download all scripts from RiboFootPrint GitHub repository","title":"Scripts"},{"location":"RiboFootPrint/init/#index","text":"You can link the pre-made index folder for hg19 in the working directory: ln -s /mnt/rnabl-work/Guiblet/CCRRBL12/index ./","title":"Index"},{"location":"RiboFootPrint/init/#set-parameters","text":"Update the file: RiboFootPrint.parameters.yaml","title":"Set Parameters"},{"location":"RiboFootPrint/run/","text":"Run pipeline: \u00b6 Simply submit the pipeline as a slurm job: sbatch RiboFootPrint.sh If your previous run was not completed and you wish to resume, resubmit the slurm job as: sbatch RiboFootPrint.sh -resume Running this pipeline will download a Docker container in a folder named \"work\". To remove the container, delete the folder. rm -r work","title":"Run"},{"location":"RiboFootPrint/run/#run-pipeline","text":"Simply submit the pipeline as a slurm job: sbatch RiboFootPrint.sh If your previous run was not completed and you wish to resume, resubmit the slurm job as: sbatch RiboFootPrint.sh -resume Running this pipeline will download a Docker container in a folder named \"work\". To remove the container, delete the folder. rm -r work","title":"Run pipeline:"},{"location":"eCLIP/eCLIP_workflow/","text":"eCLIP Workflow \u00b6 The eCLIP Workflow implemented here was designed to run on a High-performance Cluster such as Biowulf or FRCE . The heart of the workflow analysis uses the eCLIP workflow from Yeo Lab's. Detailed information on required software can be found using the following links: Slurm workload Manager Environmental Modules SingularityCE git eCLIP from Yeo Lab. MultiQC from Seqera. The first four items are typically provided by a High-performance Cluster such as Biowulf or FRCE . Setting Up eCLIP Workflow \u00b6 Create an Environment to Run eCLIP Workflow on FRCE \u00b6 module load mamba mamba create -n cwl_env conda-forge::tabulate conda-forge::cwltool matplotlib mamba activate cwl_env pip install multiqc multiqc --version Download Yeo Lab's GitHub Repository \u00b6 SCRIPT_DIR = /home/ $USER /eCLIP_WF mkdir $SCRIPT_DIR cd $SCRIPT_DIR git clone git@github.com:YeoLab/eCLIP.git ECLIP_DIR = $SCRIPT_DIR /eCLIP ls -l $ECLIP_DIR Download custom scripts to run the pipeline on biowulf / FRCE \u00b6 env.yml post_process_eCLIP.py run.sh summarize_merge_peaks_wf.py Setting Up Required Files and Directories \u00b6 Directory locations should be based on the preferences of the user. As an example, we provide the following set up below as set up on FRCE. It is recommend that you separate the results or runs directory from the data directory. A full description of required files is located with links below: Prerequisite files Description of the manifest File DATA_DIR = /home/ $USER /Data RUN_DIR = /scratch/cluster_scratch/ $USER /eCLIP_run mkdir $DATA_DIR mkdir $RUN_DIR Create a manifest directory and manifest file as required by eCLIP. MANIFEST_DIR = $DATA_DIR /manifests mkdir $MANIFEST_DIR # For single-end reads copy, modify and rename file cp $ECLIP_DIR /example/single_end_clip.yaml $MANIFEST_DIR / Modify the Run Script \u00b6 Modify the run.sh script file. Running Workflow \u00b6 The workflow has two major parts: Run YeoLab's eCLIP . Followed by YeoLab's merge_peaks sbatch run.sh Results \u00b6 A full description of results files are located with eCLIP Outputs and merge_peaks Outputs . We provide two summary html files are generated: eCLIP_ReadMeSummary.html summary results file as a starting point for exploration of your results of eCLIP , and eCLIP_MergePeaks_ReadMeSummary.html for merge_peaks .","title":"Workflow"},{"location":"eCLIP/eCLIP_workflow/#eclip-workflow","text":"The eCLIP Workflow implemented here was designed to run on a High-performance Cluster such as Biowulf or FRCE . The heart of the workflow analysis uses the eCLIP workflow from Yeo Lab's. Detailed information on required software can be found using the following links: Slurm workload Manager Environmental Modules SingularityCE git eCLIP from Yeo Lab. MultiQC from Seqera. The first four items are typically provided by a High-performance Cluster such as Biowulf or FRCE .","title":"eCLIP Workflow"},{"location":"eCLIP/eCLIP_workflow/#setting-up-eclip-workflow","text":"","title":"Setting Up eCLIP Workflow"},{"location":"eCLIP/eCLIP_workflow/#create-an-environment-to-run-eclip-workflow-on-frce","text":"module load mamba mamba create -n cwl_env conda-forge::tabulate conda-forge::cwltool matplotlib mamba activate cwl_env pip install multiqc multiqc --version","title":"Create an Environment to Run eCLIP Workflow on FRCE"},{"location":"eCLIP/eCLIP_workflow/#download-yeo-labs-github-repository","text":"SCRIPT_DIR = /home/ $USER /eCLIP_WF mkdir $SCRIPT_DIR cd $SCRIPT_DIR git clone git@github.com:YeoLab/eCLIP.git ECLIP_DIR = $SCRIPT_DIR /eCLIP ls -l $ECLIP_DIR","title":"Download Yeo Lab's GitHub Repository"},{"location":"eCLIP/eCLIP_workflow/#download-custom-scripts-to-run-the-pipeline-on-biowulf-frce","text":"env.yml post_process_eCLIP.py run.sh summarize_merge_peaks_wf.py","title":"Download custom scripts to run the pipeline on biowulf / FRCE"},{"location":"eCLIP/eCLIP_workflow/#setting-up-required-files-and-directories","text":"Directory locations should be based on the preferences of the user. As an example, we provide the following set up below as set up on FRCE. It is recommend that you separate the results or runs directory from the data directory. A full description of required files is located with links below: Prerequisite files Description of the manifest File DATA_DIR = /home/ $USER /Data RUN_DIR = /scratch/cluster_scratch/ $USER /eCLIP_run mkdir $DATA_DIR mkdir $RUN_DIR Create a manifest directory and manifest file as required by eCLIP. MANIFEST_DIR = $DATA_DIR /manifests mkdir $MANIFEST_DIR # For single-end reads copy, modify and rename file cp $ECLIP_DIR /example/single_end_clip.yaml $MANIFEST_DIR /","title":"Setting Up Required Files and Directories"},{"location":"eCLIP/eCLIP_workflow/#modify-the-run-script","text":"Modify the run.sh script file.","title":"Modify the Run Script"},{"location":"eCLIP/eCLIP_workflow/#running-workflow","text":"The workflow has two major parts: Run YeoLab's eCLIP . Followed by YeoLab's merge_peaks sbatch run.sh","title":"Running Workflow"},{"location":"eCLIP/eCLIP_workflow/#results","text":"A full description of results files are located with eCLIP Outputs and merge_peaks Outputs . We provide two summary html files are generated: eCLIP_ReadMeSummary.html summary results file as a starting point for exploration of your results of eCLIP , and eCLIP_MergePeaks_ReadMeSummary.html for merge_peaks .","title":"Results"},{"location":"iCLIP/descript/","text":"iCLIP \u00b6 This pipelines process and analyzes iCLIP data from raw fasqt files. Steps of the pipeline: For more questions about this pipeline, you may contact Wilfried Guiblet .","title":"Description"},{"location":"iCLIP/descript/#iclip","text":"This pipelines process and analyzes iCLIP data from raw fasqt files. Steps of the pipeline: For more questions about this pipeline, you may contact Wilfried Guiblet .","title":"iCLIP"},{"location":"iCLIP/init/","text":"Scripts \u00b6 Download nextflow.parameters.yaml in your directory. Update the parameters to fit your experiment.","title":"Initialize"},{"location":"iCLIP/init/#scripts","text":"Download nextflow.parameters.yaml in your directory. Update the parameters to fit your experiment.","title":"Scripts"},{"location":"iCLIP/run/","text":"Run pipeline: \u00b6 On biowulf, Load the latest version of the pipeline: export PATH = $PATH :/data/RBL_NCI/iCLIP/latest/ Run the pipeline with genomic coordinates: sbatch iCLIP_latest.sh \"your_directory\" After running with genomic coordinates, you can also run for transcriptomic coordinates: sbatch iCLIP_Transcriptome_latest.sh \"your_directory\" You can add the option -resume to resume a failed/interrupted run.","title":"Run"},{"location":"iCLIP/run/#run-pipeline","text":"On biowulf, Load the latest version of the pipeline: export PATH = $PATH :/data/RBL_NCI/iCLIP/latest/ Run the pipeline with genomic coordinates: sbatch iCLIP_latest.sh \"your_directory\" After running with genomic coordinates, you can also run for transcriptomic coordinates: sbatch iCLIP_Transcriptome_latest.sh \"your_directory\" You can add the option -resume to resume a failed/interrupted run.","title":"Run pipeline:"}]}